{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Depth × Width Sweep (MNIST)\n",
        "We vary network **depth** (number of hidden layers) and **width** (neurons per layer) and track\n",
        "train/val/test accuracy + loss to study generalization and overfitting."
      ],
      "metadata": {
        "id": "ZiQbXNXHUiIv"
      },
      "id": "ZiQbXNXHUiIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set-up + Data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load + preprocess\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = (X_train / 255.0).reshape(-1, 784)\n",
        "X_test  = (X_test / 255.0).reshape(-1, 784)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "TkHv2w-zUjI4"
      },
      "id": "TkHv2w-zUjI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Model\n",
        "def build_mlp(depth: int, width: int, lr: float = 0.001):\n",
        "    \"\"\"\n",
        "    depth = number of hidden layers\n",
        "    width = neurons per hidden layer\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(width, activation=\"relu\", input_shape=(784,)))\n",
        "    for _ in range(depth - 1):\n",
        "        model.add(Dense(width, activation=\"relu\"))\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "JfEewVDPUwRP"
      },
      "id": "JfEewVDPUwRP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model: Hyperparameter Tuning Block\n",
        "depths = [1, 2, 3, 4]\n",
        "widths = [64, 128, 256, 512]\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH  = 128\n",
        "LR     = 0.001\n",
        "\n",
        "results = []  # list of dicts\n",
        "\n",
        "for d in depths:\n",
        "    for w in widths:\n",
        "        print(f\"\\nTraining: depth={d}, width={w}\")\n",
        "        model = build_mlp(depth=d, width=w, lr=LR)\n",
        "        hist = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH,\n",
        "            verbose=0\n",
        "        )\n",
        "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "        row = {\n",
        "            \"depth\": d,\n",
        "            \"width\": w,\n",
        "            \"train_acc\": float(hist.history[\"accuracy\"][-1]),\n",
        "            \"val_acc\": float(hist.history[\"val_accuracy\"][-1]),\n",
        "            \"test_acc\": float(test_acc),\n",
        "            \"train_loss\": float(hist.history[\"loss\"][-1]),\n",
        "            \"val_loss\": float(hist.history[\"val_loss\"][-1]),\n",
        "            \"test_loss\": float(test_loss),\n",
        "        }\n",
        "        results.append(row)\n",
        "        print(f\"val_acc={row['val_acc']:.4f}, test_acc={row['test_acc']:.4f}\")"
      ],
      "metadata": {
        "id": "QdTA3DSFV_pw"
      },
      "id": "QdTA3DSFV_pw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put into a table + find best\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results).sort_values([\"test_acc\"], ascending=False)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "EfzpYqTyWPFT"
      },
      "id": "EfzpYqTyWPFT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot: test accuracy by width for each depth\n",
        "plt.figure()\n",
        "for d in depths:\n",
        "    sub = df[df[\"depth\"] == d].sort_values(\"width\")\n",
        "    plt.plot(sub[\"width\"], sub[\"test_acc\"], marker=\"o\", label=f\"depth={d}\")\n",
        "plt.xlabel(\"Width (neurons per layer)\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"Depth × Width Sweep: Test Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ilUqq9xnWmvB"
      },
      "id": "ilUqq9xnWmvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot: generalization gap (train - val)\n",
        "plt.figure()\n",
        "for d in depths:\n",
        "    sub = df[df[\"depth\"] == d].sort_values(\"width\")\n",
        "    gap = sub[\"train_acc\"].values - sub[\"val_acc\"].values\n",
        "    plt.plot(sub[\"width\"], gap, marker=\"o\", label=f\"depth={d}\")\n",
        "plt.xlabel(\"Width (neurons per layer)\")\n",
        "plt.ylabel(\"Train - Val Accuracy Gap\")\n",
        "plt.title(\"Overfitting Signal: Generalization Gap\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v-LmrsWeWrFq"
      },
      "id": "v-LmrsWeWrFq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}